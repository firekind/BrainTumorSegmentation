{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import SimpleITK as sitk\n",
    "from scipy.ndimage import zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(\"data/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ds = tf.data.Dataset.list_files(str(dataset_path/'*GG/*/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "b'data/train/HGG/Brats18_TCIA02_290_1'\nb'data/train/HGG/Brats18_CBICA_APR_1'\n"
    }
   ],
   "source": [
    "for f in list_ds.take(2):\n",
    "    print(f.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(img: np.ndarray, shape: Tuple[int, int, int], mode: str = 'constant') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Wrapper for scipy.ndimage.zoom suited for MRI images.\n",
    "    \n",
    "    Args:\n",
    "        img (np.ndarray): The image to resize.\n",
    "        shape (Tuple[int, int, int]): The shape to resize to.\n",
    "        mode (str, optional): The mode to use while resizing. Defaults to 'constant'.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: The resized image.\n",
    "    \"\"\"\n",
    "    assert len(shape) >= 3, \"Output shape cannot have more than 3 dimensions\"\n",
    "\n",
    "    orig_shape = img.shape\n",
    "    factors = (\n",
    "        shape[0]/orig_shape[0],\n",
    "        shape[1]/orig_shape[1], \n",
    "        shape[2]/orig_shape[2]\n",
    "    )\n",
    "    \n",
    "    # Resize to the given shape\n",
    "    return zoom(img, factors, mode=mode)\n",
    "\n",
    "def load_nii(paths: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reads the .nii files specified by their paths and returns the data.\n",
    "    \n",
    "    Args:\n",
    "        paths (List[str]): The list of paths to the files to read.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: A numpy array containing the data of each individual file.\n",
    "    \"\"\"\n",
    "\n",
    "    # creating the array to store the data\n",
    "    data: np.ndarray = None\n",
    "\n",
    "    # for every path in the list of given paths\n",
    "    for i in range(len(paths)):\n",
    "\n",
    "        # read the file\n",
    "        d = sitk.GetArrayFromImage(sitk.ReadImage(paths[i].decode('utf-8')))\n",
    "\n",
    "        # allocating the variable to store the data if it has not been allocated\n",
    "        # already\n",
    "        if data is None:\n",
    "            data = np.zeros((len(paths), *d.shape), dtype=np.float32)\n",
    "\n",
    "        # assigning the file contents to the data array\n",
    "        data[i] = d\n",
    "\n",
    "    # returning the data\n",
    "    return data\n",
    "\n",
    "def get_files(parent_dir: bytes) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Gets the contents of the directory.\n",
    "    \n",
    "    Args:\n",
    "        parent_dir (bytes): The directory to get the contents of.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[List[str], List[str]]: An array of the paths to the data files, \n",
    "        and an array to the path of the segmentation mask (label).\n",
    "    \"\"\"\n",
    "\n",
    "    # getting the list of files in the directory\n",
    "    files = os.listdir(parent_dir)\n",
    "\n",
    "    # filtering out the data files from the list of files in the directory\n",
    "    data_files = [os.path.join(parent_dir, i) for i in files if b\"seg\" not in i]\n",
    "\n",
    "    # filtering out the segmentation mask file (label) from the list of files in the directory\n",
    "    seg_file = [os.path.join(parent_dir, i) for i in files if b\"seg\" in i]\n",
    "    \n",
    "    # returning the two\n",
    "    return data_files, seg_file\n",
    "\n",
    "\n",
    "def preprocess_data(images: np.ndarray, out_shape: Tuple[int, int, int] = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocess the input data.\n",
    "    \n",
    "    Args:\n",
    "        images (np.ndarray): The list of images to preprocess\n",
    "        out_shape (Tuple[int, int, int, int], optional): The output shape of the image, if resizing\n",
    "        is requried. Defaults to None.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: The array containing the resultant images.\n",
    "    \"\"\"\n",
    "\n",
    "    # creating the array to store the resultant images\n",
    "    out_imgs = np.zeros((len(images), *out_shape), dtype=np.float32)\n",
    "\n",
    "    # for every images\n",
    "    for i, img in enumerate(images):\n",
    "\n",
    "        # if there is a need to resize the image, resize the image.\n",
    "        if out_shape is not None:\n",
    "            _img = resize(img, out_shape)\n",
    "        else:\n",
    "            _img = img\n",
    "\n",
    "        # normalizing the image\n",
    "        mean = _img.mean()\n",
    "        std = _img.std()\n",
    "        out_imgs[i] = (_img - mean) / std\n",
    "    \n",
    "    return out_imgs\n",
    "\n",
    "def preprocess_label(seg_mask: np.ndarray, out_shape: Tuple[int, int, int] = None, mode: str = 'nearest') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Separates out the 3 labels from the segmentation provided, namely:\n",
    "    GD-enhancing tumor (ET — label 4), the peritumoral edema (ED — label 2))\n",
    "    and the necrotic and non-enhancing tumor core (NCR/NET — label 1)\n",
    "    \n",
    "    Args:\n",
    "        seg_mask (np.ndarray): The numpy array containing the data of the label.\n",
    "        out_shape (Tuple[int, int, int], optional): The shape to resize to. Defaults to None.\n",
    "        mode (str, optional): The mode to use while resizing. Defaults to 'nearest'.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: The resultant image.\n",
    "    \"\"\"\n",
    "\n",
    "    # extracting the labels from the segmentation mask\n",
    "    ncr = seg_mask == 1  # Necrotic and Non-Enhancing Tumor (NCR/NET)\n",
    "    ed = seg_mask == 2  # Peritumoral Edema (ED)\n",
    "    et = seg_mask == 4  # GD-enhancing Tumor (ET)\n",
    "    \n",
    "    # resizing if required\n",
    "    if out_shape is not None:\n",
    "        ncr = resize(ncr, out_shape, mode=mode)\n",
    "        ed = resize(ed, out_shape, mode=mode)\n",
    "        et = resize(et, out_shape, mode=mode)\n",
    "\n",
    "    return np.array([ncr, ed, et], dtype=np.uint8)\n",
    "\n",
    "def process_paths(parent_dir: str, out_shape: Tuple[int, int, int] = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Loads the images from the parent directory, processes them and returns the processed images.\n",
    "    \n",
    "    Args:\n",
    "        parent_dir (str): The path to the parent directory.\n",
    "        out_shape (Tuple[int, int, int], optional): The shape to resize to. Defaults to None.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray]: First, the data, second, the labels.\n",
    "    \"\"\"\n",
    "\n",
    "    data_files, seg_file = get_files(parent_dir)\n",
    "    data: tf.Tensor = load_nii(data_files)\n",
    "    seg: tf.Tensor = load_nii(seg_file)[0]\n",
    "    \n",
    "    data = preprocess_data(data, out_shape)\n",
    "    label = preprocess_label(seg, out_shape)\n",
    "\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = list_ds.map(lambda x: tf.numpy_function(process_paths, [x, (80, 96, 64)], [tf.float32, tf.uint8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "x: (4, 80, 96, 64)\ny: (3, 80, 96, 64)\n6.46 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
    }
   ],
   "source": [
    "%%timeit -r1 -n1\n",
    "for x, y in res:\n",
    "    print(f\"x: {x.shape}\")\n",
    "    print(f\"y: {y.shape}\")  \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}